#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•GLM-4.6 APIè°ƒç”¨
"""

import asyncio
import json
import time
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

async def test_glm_api():
    """ç›´æ¥æµ‹è¯•GLM API"""
    print("ğŸš€ ç›´æ¥æµ‹è¯•GLM-4.6 APIè°ƒç”¨...")

    # å¯¼å…¥LLMæœåŠ¡
    from services.llm.llm_service import get_llm_service, LLMProvider

    # åŠ è½½é…ç½®
    import yaml
    config_path = Path("config/config.yaml")
    if not config_path.exists():
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        return False

    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")

    # åˆå§‹åŒ–LLMæœåŠ¡
    llm_service = get_llm_service(config)

    # æµ‹è¯•æ–‡æœ¬
    test_text = """åœ¨AIç¼–ç¨‹å·¥å…·ç«äº‰ç™½çƒ­åŒ–çš„å½“ä¸‹ï¼ŒOpenAIæ¨å‡ºçš„Codexç¼–ç¨‹åŠ©æ‰‹å‡­å€Ÿ"æœ¬åœ°å®‰å…¨è¿è¡Œ"ã€"ChatGPTæ·±åº¦é›†æˆ"ã€"å…¨å·¥å…·é“¾è¦†ç›–"ä¸‰å¤§æ ¸å¿ƒä¼˜åŠ¿ï¼Œè¿…é€Ÿåœ¨GitHubç‹‚æ½4ä¸‡æ˜Ÿæ ‡ï¼Œæˆä¸ºå¼€å‘è€…çƒ­è®®çš„ç„¦ç‚¹ã€‚è¿™æ¬¾å·¥å…·æ­è½½GPT-5-Codexæ¨¡å‹ï¼Œèƒ½åƒä¸“ä¸šç¨‹åºå‘˜èˆ¬è¿ç»­7å°æ—¶è¿­ä»£å¤æ‚é¡¹ç›®ã€ä¿®å¤Bugã€è¿è¡Œæµ‹è¯•ï¼Œå½»åº•æ”¹å˜ä¼ ç»Ÿç¼–ç¨‹çš„ä½æ•ˆæµç¨‹ã€‚"""

    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")

    # æµ‹è¯•1: GLM-4.6
    print("\nğŸ§ª æµ‹è¯•1: GLM-4.6")
    try:
        start_time = time.time()
        result = await llm_service.optimize_text_for_video(test_text, LLMProvider.GLM)
        end_time = time.time()

        print(f"âœ… GLM-4.6 æˆåŠŸ!")
        print(f"ğŸ“Š å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ“„ ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
        print(f"ğŸ¯ ç»“æœé¢„è§ˆ: {result[:150]}...")

        return True

    except Exception as e:
        print(f"âŒ GLM-4.6 å¤±è´¥: {e}")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«429é”™è¯¯ä¿¡æ¯
        if "429" in str(e) or "Too Many Requests" in str(e):
            print("âš ï¸  ç¡®è®¤æ˜¯429é€Ÿç‡é™åˆ¶é”™è¯¯")
            return False
        else:
            print("âš ï¸  å…¶ä»–ç±»å‹çš„é”™è¯¯")
            return False

async def test_multiple_requests():
    """æµ‹è¯•å¤šä¸ªè¿ç»­è¯·æ±‚"""
    print("\nğŸ§ª æµ‹è¯•2: å¤šä¸ªè¿ç»­è¯·æ±‚ï¼ˆè§¦å‘é‡è¯•æœºåˆ¶ï¼‰")

    from services.llm.llm_service import get_llm_service, LLMProvider
    import yaml

    config_path = Path("config/config.yaml")
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    llm_service = get_llm_service(config)

    success_count = 0
    fail_count = 0
    fallback_count = 0

    for i in range(3):
        print(f"\nğŸ“¤ ç¬¬{i+1}æ¬¡è¯·æ±‚...")

        test_text = f"ç¬¬{i+1}æ¬¡æµ‹è¯•ï¼šGLM-4.6æ–‡æœ¬ä¼˜åŒ–åŠŸèƒ½ï¼ŒéªŒè¯é‡è¯•å’Œé™çº§æœºåˆ¶æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚"

        try:
            start_time = time.time()
            result = await llm_service.optimize_text_for_video(test_text, LLMProvider.GLM)
            end_time = time.time()

            print(f"âœ… ç¬¬{i+1}æ¬¡è¯·æ±‚æˆåŠŸ")
            print(f"ğŸ“Š å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“„ ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
            success_count += 1

        except Exception as e:
            print(f"âŒ ç¬¬{i+1}æ¬¡è¯·æ±‚å¤±è´¥: {e}")
            fail_count += 1

    print(f"\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸ: {success_count}")
    print(f"âŒ å¤±è´¥: {fail_count}")
    print(f"ğŸ”„ æ€»è®¡: {success_count + fail_count}")

    return success_count > 0

async def test_http_api():
    """é€šè¿‡HTTP APIæµ‹è¯•"""
    print("\nğŸ§ª æµ‹è¯•3: HTTP APIè°ƒç”¨")

    # æ£€æŸ¥æ˜¯å¦æœ‰åç«¯æœåŠ¡å™¨è¿è¡Œ
    try:
        import urllib.request
        import urllib.error
        import socket

        # å°è¯•è¿æ¥åˆ°åç«¯
        try:
            data = json.dumps({'text': 'test', 'provider': 'glm'}).encode('utf-8')
            req = urllib.request.Request(
                'http://localhost:9000/api/v1/llm/optimize-text',
                data=data,
                headers={'Content-Type': 'application/json'}
            )

            with urllib.request.urlopen(req, timeout=5) as response:
                if response.status == 200:
                    print("âœ… åç«¯APIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
                    # æµ‹è¯•å®é™…è¯·æ±‚
                    await test_backend_api()
                    return True
        except Exception as e:
            print(f"âŒ åç«¯APIæœåŠ¡å™¨æœªè¿è¡Œ: {e}")
            return False

    except Exception as e:
        print(f"âŒ HTTPæµ‹è¯•å¼‚å¸¸: {e}")
        return False

async def test_backend_api():
    """æµ‹è¯•åç«¯API"""
    test_data = {
        'text': 'æµ‹è¯•åç«¯APIçš„GLM-4.6æ–‡æœ¬ä¼˜åŒ–åŠŸèƒ½ï¼ŒéªŒè¯å‰ç«¯åˆ°åç«¯çš„å®Œæ•´æµç¨‹ã€‚',
        'provider': 'glm'
    }

    try:
        print("ğŸ“¤ å‘é€HTTP APIè¯·æ±‚...")
        start_time = time.time()

        import urllib.request
        import urllib.error

        data = json.dumps(test_data).encode('utf-8')
        req = urllib.request.Request(
            'http://localhost:9000/api/v1/llm/optimize-text',
            data=data,
            headers={'Content-Type': 'application/json'}
        )

        with urllib.request.urlopen(req, timeout=30) as response:
            end_time = time.time()

            print(f"ğŸ“¥ HTTPå“åº”çŠ¶æ€: {response.status}")
            print(f"ğŸ“Š å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")

            if response.status == 200:
                response_data = response.read().decode('utf-8')
                data = json.loads(response_data)
                print("âœ… HTTP APIè°ƒç”¨æˆåŠŸ")
                print(f"ğŸ“„ è¿”å›æ•°æ®: {json.dumps(data, ensure_ascii=False, indent=2)}")
                return True
            else:
                print(f"âŒ HTTP APIè°ƒç”¨å¤±è´¥: {response.status}")
                return False

    except Exception as e:
        print(f"ğŸš¨ HTTP APIè¯·æ±‚å¼‚å¸¸: {e}")
        return False

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ GLM-4.6 APIæµ‹è¯•")
    print("=" * 50)

    results = []

    # æµ‹è¯•1: ç›´æ¥LLMæœåŠ¡
    result1 = await test_glm_api()
    results.append(("ç›´æ¥LLMæœåŠ¡æµ‹è¯•", result1))

    # æµ‹è¯•2: å¤šä¸ªè¯·æ±‚
    result2 = await test_multiple_requests()
    results.append(("å¤šè¯·æ±‚é‡è¯•æµ‹è¯•", result2))

    # æµ‹è¯•3: HTTP API
    result3 = await test_http_api()
    results.append(("HTTP APIæµ‹è¯•", result3))

    print("\n" + "=" * 50)
    print("ğŸ¯ æµ‹è¯•æ€»ç»“:")

    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")

    passed = sum(1 for _, result in results if result)
    total = len(results)

    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GLM-4.6ä¿®å¤æˆåŠŸï¼")
    elif passed > 0:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼ŒåŸºæœ¬åŠŸèƒ½å¯ç”¨")
    else:
        print("ğŸš¨ æ‰€æœ‰æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤")

if __name__ == "__main__":
    asyncio.run(main())