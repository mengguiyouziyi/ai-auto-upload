"""
æ™ºèƒ½çˆ¬è™«æœåŠ¡ - åŸºäºçœŸå®æŠ“å–åŠŸèƒ½
"""

import asyncio
import aiohttp
import json
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from pathlib import Path


class SpiderService:
    """æ™ºèƒ½çˆ¬è™«æœåŠ¡ç±»"""

    def __init__(self, config: Dict = None):
        """åˆå§‹åŒ–çˆ¬è™«æœåŠ¡"""
        self.config = config or {}

        # æ”¯æŒçš„å¹³å°é…ç½®
        self.platforms = {
            "csdn": {
                "name": "CSDN",
                "available": True,
                "base_url": "https://blog.csdn.net"
            },
            "juejin": {
                "name": "æ˜é‡‘",
                "available": True,
                "base_url": "https://juejin.cn"
            },
            "zhihu": {
                "name": "çŸ¥ä¹",
                "available": True,
                "base_url": "https://www.zhihu.com"
            },
            "toutiao": {
                "name": "ä»Šæ—¥å¤´æ¡",
                "available": True,
                "base_url": "https://www.toutiao.com"
            },
            "xiaohongshu": {
                "name": "å°çº¢ä¹¦",
                "available": True,
                "base_url": "https://www.xiaohongshu.com"
            },
            "weibo": {
                "name": "å¾®åš",
                "available": True,
                "base_url": "https://weibo.com"
            }
        }

        # é€šç”¨è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }

        print("ğŸ•·ï¸ æ™ºèƒ½çˆ¬è™«æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    async def crawl_article(self, url: str, mode: str = "content", depth: int = 1,
                          filters: List[str] = None, delay: float = 1.0) -> Dict[str, Any]:
        """æŠ“å–æ–‡ç« å†…å®¹"""
        print(f"ğŸ¯ å¼€å§‹æŠ“å–: {url}")
        print(f"   æ¨¡å¼: {mode}")
        print(f"   æ·±åº¦: {depth}")
        print(f"   è¿‡æ»¤å™¨: {filters or []}")

        # è‡ªåŠ¨è¯†åˆ«å¹³å°
        platform = self._identify_platform(url)
        print(f"   è¯†åˆ«å¹³å°: {platform} ({self.platforms.get(platform, {}).get('name', 'é€šç”¨')})")

        try:
            # æ·»åŠ å»¶è¿Ÿ
            if delay > 0:
                await asyncio.sleep(delay)

            # è·å–ç½‘é¡µå†…å®¹
            content = await self._fetch_content(url)
            if not content:
                return self._create_error_response(url, platform, "æ— æ³•è·å–é¡µé¢å†…å®¹")

            # è§£æå†…å®¹
            article_data = await self._parse_content(content, url, platform, mode)

            # åº”ç”¨è¿‡æ»¤å™¨
            if filters:
                article_data = self._apply_filters(article_data, filters)

            # ç»Ÿè®¡ä¿¡æ¯
            article_data.update({
                "crawl_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "platform": platform,
                "mode": mode,
                "depth": depth,
                "filters": filters or [],
                "word_count": len(article_data.get("content", "")),
                "image_count": len(article_data.get("images", [])),
                "link_count": len(article_data.get("links", []))
            })

            print(f"âœ… æŠ“å–å®Œæˆ: {article_data.get('title', 'unknown')}")
            print(f"   å­—æ•°: {article_data['word_count']}")
            print(f"   å›¾ç‰‡: {article_data['image_count']}å¼ ")
            print(f"   é“¾æ¥: {article_data['link_count']}ä¸ª")

            return article_data

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {str(e)}")
            return self._create_error_response(url, platform, str(e))

    def _identify_platform(self, url: str) -> str:
        """æ ¹æ®URLè¯†åˆ«å¹³å°"""
        domain = urlparse(url).netloc.lower()

        for platform_id, platform_info in self.platforms.items():
            if platform_id in domain or platform_info["base_url"].split("//")[1] in domain:
                return platform_id

        return "general"

    async def _fetch_content(self, url: str) -> Optional[str]:
        """è·å–ç½‘é¡µå†…å®¹"""
        timeout = aiohttp.ClientTimeout(total=30)

        try:
            async with aiohttp.ClientSession(headers=self.headers, timeout=timeout) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        print(f"ğŸ“„ é¡µé¢è·å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                        return content
                    else:
                        print(f"âš ï¸ é¡µé¢è·å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return None

        except asyncio.TimeoutError:
            print(f"â° è¯·æ±‚è¶…æ—¶: {url}")
            return None
        except Exception as e:
            print(f"ğŸŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            return None

    async def _parse_content(self, html: str, url: str, platform: str, mode: str) -> Dict[str, Any]:
        """è§£æé¡µé¢å†…å®¹"""
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # æ ¹æ®å¹³å°é€‰æ‹©è§£æç­–ç•¥
            if platform == "csdn":
                return await self._parse_csdn(soup, url, mode)
            elif platform == "juejin":
                return await self._parse_juejin(soup, url, mode)
            elif platform == "zhihu":
                return await self._parse_zhihu(soup, url, mode)
            elif platform == "toutiao":
                return await self._parse_toutiao(soup, url, mode)
            elif platform == "xiaohongshu":
                return await self._parse_xiaohongshu(soup, url, mode)
            else:
                return await self._parse_general(soup, url, mode)

        except Exception as e:
            print(f"ğŸ” å†…å®¹è§£æå¤±è´¥: {str(e)}")
            return self._create_error_response(url, platform, f"è§£æå¤±è´¥: {str(e)}")

    async def _parse_csdn(self, soup: BeautifulSoup, url: str, mode: str) -> Dict[str, Any]:
        """è§£æCSDNæ–‡ç« """
        # æ ‡é¢˜
        title_elem = soup.find('h1', class_='article-title') or soup.find('h1') or soup.find('title')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"

        # ä½œè€…
        author_elem = soup.find('a', class_='author') or soup.find('span', class_='author-name')
        author = author_elem.get_text().strip() if author_elem else "æœªçŸ¥ä½œè€…"

        # å‘å¸ƒæ—¶é—´
        time_elem = soup.find('span', class_='time') or soup.find('div', class_='article-info-box')
        publish_time = time_elem.get_text().strip() if time_elem else datetime.now().strftime('%Y-%m-%d')

        # å†…å®¹
        content_elem = soup.find('div', class_='article-content') or soup.find('div', id='content_views') or soup.find('article')
        content = self._extract_text(content_elem) if content_elem else ""

        # å›¾ç‰‡å’Œé“¾æ¥
        images = self._extract_images(soup, url)
        links = self._extract_links(soup, url)

        # å…³é”®è¯
        keywords = self._extract_keywords(soup)

        return {
            "title": title,
            "author": author,
            "publish_time": publish_time,
            "content": content,
            "images": images,
            "links": links,
            "keywords": keywords,
            "url": url,
            "platform": "csdn"
        }

    async def _parse_juejin(self, soup: BeautifulSoup, url: str, mode: str) -> Dict[str, Any]:
        """è§£ææ˜é‡‘æ–‡ç« """
        # æ ‡é¢˜
        title_elem = soup.find('h1', class_='article-title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"

        # ä½œè€…
        author_elem = soup.find('span', class_='username') or soup.find('a', class_='user-name')
        author = author_elem.get_text().strip() if author_elem else "æœªçŸ¥ä½œè€…"

        # å‘å¸ƒæ—¶é—´
        time_elem = soup.find('span', class_='meta-time') or soup.find('time')
        publish_time = time_elem.get_text().strip() if time_elem else datetime.now().strftime('%Y-%m-%d')

        # å†…å®¹
        content_elem = soup.find('div', class_='article-content') or soup.find('div', class_='markdown-body')
        content = self._extract_text(content_elem) if content_elem else ""

        # å›¾ç‰‡å’Œé“¾æ¥
        images = self._extract_images(soup, url)
        links = self._extract_links(soup, url)

        # å…³é”®è¯
        keywords = self._extract_keywords(soup)

        return {
            "title": title,
            "author": author,
            "publish_time": publish_time,
            "content": content,
            "images": images,
            "links": links,
            "keywords": keywords,
            "url": url,
            "platform": "juejin"
        }

    async def _parse_zhihu(self, soup: BeautifulSoup, url: str, mode: str) -> Dict[str, Any]:
        """è§£æçŸ¥ä¹æ–‡ç« """
        # æ ‡é¢˜
        title_elem = soup.find('h1', class_='QuestionHeader-title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"

        # ä½œè€…
        author_elem = soup.find('span', class_='UserLink-link') or soup.find('a', class_='author-link')
        author = author_elem.get_text().strip() if author_elem else "æœªçŸ¥ä½œè€…"

        # å‘å¸ƒæ—¶é—´
        time_elem = soup.find('span', class_='ContentItem-time') or soup.find('time')
        publish_time = time_elem.get_text().strip() if time_elem else datetime.now().strftime('%Y-%m-%d')

        # å†…å®¹
        content_elem = soup.find('div', class_='RichText') or soup.find('div', class_='Post-RichText')
        content = self._extract_text(content_elem) if content_elem else ""

        # å›¾ç‰‡å’Œé“¾æ¥
        images = self._extract_images(soup, url)
        links = self._extract_links(soup, url)

        # å…³é”®è¯
        keywords = self._extract_keywords(soup)

        return {
            "title": title,
            "author": author,
            "publish_time": publish_time,
            "content": content,
            "images": images,
            "links": links,
            "keywords": keywords,
            "url": url,
            "platform": "zhihu"
        }

    async def _parse_toutiao(self, soup: BeautifulSoup, url: str, mode: str) -> Dict[str, Any]:
        """è§£æä»Šæ—¥å¤´æ¡æ–‡ç« """
        # æ ‡é¢˜
        title_elem = soup.find('h1', class_='article-title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"

        # ä½œè€…
        author_elem = soup.find('a', class_='user-name') or soup.find('span', class_='name')
        author = author_elem.get_text().strip() if author_elem else "æœªçŸ¥ä½œè€…"

        # å‘å¸ƒæ—¶é—´
        time_elem = soup.find('time') or soup.find('span', class_='time')
        publish_time = time_elem.get_text().strip() if time_elem else datetime.now().strftime('%Y-%m-%d')

        # å†…å®¹
        content_elem = soup.find('div', class_='article-content') or soup.find('div', class_='content')
        content = self._extract_text(content_elem) if content_elem else ""

        # å›¾ç‰‡å’Œé“¾æ¥
        images = self._extract_images(soup, url)
        links = self._extract_links(soup, url)

        # å…³é”®è¯
        keywords = self._extract_keywords(soup)

        return {
            "title": title,
            "author": author,
            "publish_time": publish_time,
            "content": content,
            "images": images,
            "links": links,
            "keywords": keywords,
            "url": url,
            "platform": "toutiao"
        }

    async def _parse_xiaohongshu(self, soup: BeautifulSoup, url: str, mode: str) -> Dict[str, Any]:
        """è§£æå°çº¢ä¹¦å†…å®¹"""
        # æ ‡é¢˜
        title_elem = soup.find('div', class_='title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"

        # ä½œè€…
        author_elem = soup.find('span', class_='username') or soup.find('a', class_='author')
        author = author_elem.get_text().strip() if author_elem else "æœªçŸ¥ä½œè€…"

        # å‘å¸ƒæ—¶é—´
        time_elem = soup.find('span', class_='publish-time') or soup.find('time')
        publish_time = time_elem.get_text().strip() if time_elem else datetime.now().strftime('%Y-%m-%d')

        # å†…å®¹
        content_elem = soup.find('div', class_='content') or soup.find('div', class_='desc')
        content = self._extract_text(content_elem) if content_elem else ""

        # å›¾ç‰‡å’Œé“¾æ¥
        images = self._extract_images(soup, url)
        links = self._extract_links(soup, url)

        # å…³é”®è¯
        keywords = self._extract_keywords(soup)

        return {
            "title": title,
            "author": author,
            "publish_time": publish_time,
            "content": content,
            "images": images,
            "links": links,
            "keywords": keywords,
            "url": url,
            "platform": "xiaohongshu"
        }

    async def _parse_general(self, soup: BeautifulSoup, url: str, mode: str) -> Dict[str, Any]:
        """é€šç”¨è§£æå™¨"""
        # æ ‡é¢˜
        title_elem = soup.find('title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"

        # å°è¯•æå–ä½œè€…
        author_selectors = ['author', 'byline', 'username', 'user-name', 'writer']
        author = "æœªçŸ¥ä½œè€…"
        for selector in author_selectors:
            author_elem = soup.find('span', class_=selector) or soup.find('div', class_=selector) or soup.find('a', class_=selector)
            if author_elem:
                author = author_elem.get_text().strip()
                break

        # å°è¯•æå–å‘å¸ƒæ—¶é—´
        time_selectors = ['time', 'publish-time', 'date', 'timestamp', 'meta-time']
        publish_time = datetime.now().strftime('%Y-%m-%d')
        for selector in time_selectors:
            time_elem = soup.find('time') or soup.find('span', class_=selector) or soup.find('div', class_=selector)
            if time_elem:
                publish_time = time_elem.get_text().strip()
                break

        # å†…å®¹æå–
        content_selectors = ['content', 'article-content', 'post-content', 'entry-content', 'main-content']
        content = ""
        for selector in content_selectors:
            content_elem = soup.find('div', class_=selector) or soup.find('article') or soup.find('main')
            if content_elem:
                content = self._extract_text(content_elem)
                break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æå–bodyæ–‡æœ¬
        if not content:
            body_elem = soup.find('body')
            if body_elem:
                content = self._extract_text(body_elem)

        # å›¾ç‰‡å’Œé“¾æ¥
        images = self._extract_images(soup, url)
        links = self._extract_links(soup, url)

        # å…³é”®è¯
        keywords = self._extract_keywords(soup)

        return {
            "title": title,
            "author": author,
            "publish_time": publish_time,
            "content": content,
            "images": images,
            "links": links,
            "keywords": keywords,
            "url": url,
            "platform": "general"
        }

    def _extract_text(self, element) -> str:
        """æå–å…ƒç´ ä¸­çš„çº¯æ–‡æœ¬"""
        if not element:
            return ""

        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
        for script in element(["script", "style"]):
            script.decompose()

        # è·å–æ–‡æœ¬å†…å®¹
        text = element.get_text()

        # æ¸…ç†ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        images = []
        img_elements = soup.find_all('img')

        for img in img_elements:
            src = img.get('src') or img.get('data-src') or img.get('data-original')
            alt = img.get('alt', '')

            if src:
                # è½¬æ¢ä¸ºç»å¯¹URL
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = urljoin(base_url, src)
                elif not src.startswith(('http://', 'https://')):
                    src = urljoin(base_url, src)

                images.append({
                    "url": src,
                    "alt": alt
                })

        return images

    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """æå–é“¾æ¥ä¿¡æ¯"""
        links = []
        link_elements = soup.find_all('a', href=True)

        for link in link_elements:
            href = link.get('href')
            text = link.get_text().strip()

            if href and text:
                # è½¬æ¢ä¸ºç»å¯¹URL
                if href.startswith('/'):
                    href = urljoin(base_url, href)
                elif not href.startswith(('http://', 'https://', 'mailto:', 'tel:')):
                    href = urljoin(base_url, href)

                links.append({
                    "url": href,
                    "text": text
                })

        return links

    def _extract_keywords(self, soup: BeautifulSoup) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []

        # ä»metaæ ‡ç­¾æå–
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            content = meta_keywords.get('content', '')
            keywords.extend([kw.strip() for kw in content.split(',') if kw.strip()])

        # ä»æ ‡ç­¾äº‘æˆ–åˆ†ç±»æå–
        tag_elements = soup.find_all(['span', 'a'], class_=re.compile(r'tag|category|label'))
        for tag_elem in tag_elements:
            tag_text = tag_elem.get_text().strip()
            if tag_text and len(tag_text) < 20:  # é¿å…é•¿æ–‡æœ¬
                keywords.append(tag_text)

        return list(set(keywords))  # å»é‡

    def _apply_filters(self, data: Dict[str, Any], filters: List[str]) -> Dict[str, Any]:
        """åº”ç”¨è¿‡æ»¤å™¨"""
        if not filters:
            return data

        # è¿‡æ»¤å¹¿å‘Š
        if 'ads' in filters:
            content = data.get('content', '')
            # ç§»é™¤å¸¸è§çš„å¹¿å‘Šæ–‡æœ¬æ¨¡å¼
            ad_patterns = [
                r'å¹¿å‘Š.*?æ¨å¹¿',
                r'èµåŠ©.*?å†…å®¹',
                r'ç‚¹å‡».*?è´­ä¹°',
                r'ç«‹å³.*?ä¸‹å•',
                r'ä¼˜æƒ .*?æ´»åŠ¨'
            ]
            for pattern in ad_patterns:
                content = re.sub(pattern, '', content, flags=re.IGNORECASE)
            data['content'] = content

        # è¿‡æ»¤è„šæœ¬å’Œæ ·å¼
        if 'scripts' in filters or 'styles' in filters:
            # åœ¨æ–‡æœ¬æå–é˜¶æ®µå·²ç»å¤„ç†äº†è„šæœ¬å’Œæ ·å¼
            pass

        # è¿‡æ»¤è¯„è®º
        if 'comments' in filters:
            content = data.get('content', '')
            # ç§»é™¤å¸¸è§çš„è¯„è®ºæ¨¡å¼
            comment_patterns = [
                r'ç½‘å‹è¯´.*? ',
                r'ç”¨æˆ·è¯„è®º.*? ',
                r'å¤§å®¶è§‰å¾—.*? ',
                r'æˆ‘è§‰å¾—.*? '
            ]
            for pattern in comment_patterns:
                content = re.sub(pattern, '', content, flags=re.IGNORECASE)
            data['content'] = content

        return data

    def _create_error_response(self, url: str, platform: str, error_msg: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            "title": "æŠ“å–å¤±è´¥",
            "author": "æœªçŸ¥",
            "publish_time": datetime.now().strftime('%Y-%m-%d'),
            "content": "",
            "images": [],
            "links": [],
            "keywords": [],
            "url": url,
            "platform": platform,
            "error": error_msg,
            "crawl_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "word_count": 0,
            "image_count": 0,
            "link_count": 0
        }

    def get_supported_platforms(self) -> List[Dict[str, Any]]:
        """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
        return [
            {
                "id": platform_id,
                "name": info["name"],
                "available": info["available"],
                "base_url": info["base_url"]
            }
            for platform_id, info in self.platforms.items()
        ]

    async def batch_crawl(self, urls: List[str], **kwargs) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŠ“å–"""
        results = []

        for i, url in enumerate(urls):
            print(f"ğŸ”„ æ‰¹é‡æŠ“å–è¿›åº¦: {i+1}/{len(urls)} - {url}")

            try:
                result = await self.crawl_article(url, **kwargs)
                results.append({
                    "index": i + 1,
                    "url": url,
                    "status": "success" if result.get("error") is None else "error",
                    "data": result
                })
            except Exception as e:
                results.append({
                    "index": i + 1,
                    "url": url,
                    "status": "error",
                    "error": str(e)
                })

        return results


def get_spider_service(config: Dict = None) -> SpiderService:
    """è·å–çˆ¬è™«æœåŠ¡å®ä¾‹"""
    return SpiderService(config)